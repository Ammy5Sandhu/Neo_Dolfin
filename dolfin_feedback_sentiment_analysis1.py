# -*- coding: utf-8 -*-
"""DolFin_Feedback_Sentiment_Analysis1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4uISXYkPxlkTCYqsY3cGXQm0P4k_NWX
"""

#pip install tensorflow pandas nltk wordcloud keras-tuner

# Processing Utilities
import datetime
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Natural Language Processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

# Deep learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras import Model
from kerastuner.tuners import Hyperband  # Added for hyperparameter tuning
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping  # Added for enhanced callbacks
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Plotting utilities
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Data location, batch and image size
dataset_dir = '/content/drive/MyDrive/Colab_Notebooks/'
file_name = 'reviews.csv'

# Load the dataset into a dataframe
df = pd.read_csv(dataset_dir + file_name)
df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataframe

# Display the dataframe structure
print(f"The file {file_name} contains {df.shape[1]} features and {df.shape[0]} samples.")
print(f"The column names are: {list(df.columns)}")

# Ensure the content is a string datatype
df['content'] = df['content'].astype(str)
df.dtypes

# Map sentiments based on scores
def map_sentiment(score):
    if score < 3.5:
        return 'Bad'
    else:
        return 'Good'

df['sentiment'] = df['score'].apply(map_sentiment)

# Create a stop words set
stop_words = set(stopwords.words('english'))

# Preprocess the text in the content field
def preprocess_content(text):
    tokens = word_tokenize(text.lower())
    return ' '.join([word for word in tokens if word.isalpha() and word not in stop_words])

df['processed_text'] = df['content'].apply(preprocess_content)

# Tokenization of the processed text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['processed_text'])
sequences = tokenizer.texts_to_sequences(df['processed_text'])
padded_sequences = pad_sequences(sequences, maxlen=200)

# Mapping sentiments to integers
sentiment_label = df['sentiment'].factorize()
x_train, x_temp, y_train, y_temp = train_test_split(padded_sequences, sentiment_label[0], test_size=0.3, random_state=42)
x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)

# Define the model building function with hyperparameters
def build_model(hp):
    model = tf.keras.Sequential([
        layers.Embedding(input_dim=5000, output_dim=16, input_length=200),
        layers.Bidirectional(layers.LSTM(64)),
        layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)),
        layers.Dense(hp.Int('dense_units', min_value=10, max_value=100, step=10), activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Setup the Hyperband tuner
tuner = Hyperband(
    build_model,
    objective='val_accuracy',
    max_epochs=10,
    directory='hyperparameter_logs',
    project_name='SentimentAnalysis'
)

# Callbacks for TensorBoard and Model Checkpoint
log_dir = os.path.join("logs", "fit", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
model_checkpoint_callback = ModelCheckpoint(
    'best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

# Start the hyperparameter search
tuner.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[tensorboard_callback, model_checkpoint_callback])

# Retrieve the best model
best_model = tuner.get_best_models(num_models=1)[0]
loss, accuracy = best_model.evaluate(x_test, y_test)
print(f"Test Accuracy of the best model: {accuracy:.2f}")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir hyperparameter_logs