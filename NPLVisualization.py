# -*- coding: utf-8 -*-
"""DolFin_Transaction_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YDhVWI7KMW-VmVmTDxfxeCuOO_LYw7Rj

# DolFin Transaction Classification
This notebook is for training and testing a DolFin NLP model that will classify the transactions based on their description, the model has been developed with the intention of replacing the existing Basiq API which has provided poor classification results. The first part of this notebook is data wrangling and preparing the classes and tokenising the transaction descriptions. The dataset is very small being only 500 samples, for this reason only 60% was used for training, 20% for testing and 20% for validation to minimise over fitting. As the dataset begins to grow the model should be tested on the new data and potentially retrained.


## Visualization Section:

In addition to the development and testing of the model, this notebook includes comprehensive visualizations to assess the performance of the DolFin NLP model. These visualizations are crucial for understanding how well our model performs compared to the Basiq API and identifying areas for improvement. We have incorporated several types of graphs:

Confusion Matrix: Provides a clear picture of the model's performance with true positives, false positives, true negatives, and false negatives, allowing us to gauge the precision and recall effectively.

Accuracy and Loss Graphs: These plots track the changes in accuracy and loss throughout the training and validation phases, highlighting the model's learning progress and stability over epochs.

Precision, Recall, and F1-Score: Further metrics to evaluate the model comprehensively, offering insights into the balance between precision and recall and the harmonic mean of these metrics through the F1-score.
"""

pip install tensorflow pandas nltk wordcloud

# Processing Utilities
import datetime
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Natural Language Processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Deep learning
import tensorflow as tf
from tensorflow import keras
from keras import Model, layers

# Plotting utillities
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Data location, batch and image size
dataset_dir = '/content/drive/MyDrive/Colab_Notebooks/'
file_name = 'transaction_ut.csv'

# Load the dataset into a dataframe
df = pd.read_csv(dataset_dir + file_name)

# Shuffle the dataframe
df = df.sample(frac=1).reset_index(drop=True)

# Store the column names
labels = list(df.columns)

# Determine the shape of the data
num_samples = df.shape[0]
num_features = df.shape[1]

msg = f'The file {file_name} contains {num_features} features and {num_samples} samples \n'
msg += f'The column names are: {labels}'
print(msg)

# Ensure the description is a string datatype
df['description'] = df['description'].astype(str)

# Display the datatypes for each of the features
df.dtypes

# Use keyword search to assign a class for each of the transactions
keyword_utiliity = ['energy', 'water', 'telecommunications', 'electricity']
keyword_income = ['payroll', 'wage', 'wages']
keyword_misc = ['atm', 'tran', 'transaction', 'fee']
keyword_loans = ['morggage', 'loan']
labels  = ['utility', 'income','misc','loans']

keyword_list = [keyword_utiliity, keyword_income, keyword_misc, keyword_loans]

#
for label, keyword in zip(labels, keyword_list):
    df['class'] = df.apply(lambda row: label if any(str(keyword).lower() in str(row['description']).lower() for keyword in keyword) else row['class'], axis=1)

#
df['class'].unique()

list(df.columns)

# Create a stop words variable
stop_words = set(stopwords.words('english'))

# Preprocess the text in the content field
def preprocess_content(text):
    # Covert the text to lower case
    tokens = word_tokenize(text.lower())
    # Remove stop words from the text and ensure each token is seproate by a space
    result = ' '.join([word for word in tokens if word.isalpha() and word not in stop_words])
    return result

# Apply the preprocessing transformation
df['processed_text'] = df['description'].apply(preprocess_content)

# Tokenisation of the processed_text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['description'])
sequences = tokenizer.texts_to_sequences(df['description'])
padded_sequences = pad_sequences(sequences, maxlen=200)

# Mapping sentiments to integers
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(df['class'])
labels = to_categorical(integer_encoded)

# Specify the split into train, test, split
train_split = 0.6
test_split = 0.2
val_split = 0.2

# Determine the number of samples
train_samples = int(train_split * num_samples)
test_samples = int(test_split * num_samples)
val_samples = num_samples - test_samples - train_samples

x_train, x_temp, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)
x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)

# Create a tensorboard callback
log_dir = os.path.join("logs", "fit", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# Create a early stopping callback to prevent overfitting
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)

tf_callbacks = [tensorboard_callback, early_stopping]

num_classes = labels.shape[1]

# Develop the model
model = tf.keras.Sequential([
    layers.Embedding(5000, 16, input_length=200),
    layers.Bidirectional(layers.LSTM(64)),
    layers.Dropout(0.2),
    layers.Dense(6, activation='relu', kernel_initializer='he_uniform'),
    layers.Dense(num_classes, activation = 'softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

# Specify the maximum number of epochs
num_epochs=100
batch_size = 32

# Fit the model to the training data
history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(x_val, y_val), callbacks=tf_callbacks)

# Save a copy of the model
model.save_weights('model.h5')

# Create a function to display the training accuracy and loss
def plt_accuracy_loss(history):
    # Plot the training history
    accuracy = history.history['accuracy']
    loss = history.history['loss']
    epochs = range(len(accuracy))

    figure, ax = plt.subplots(2, 1, figsize=(12, 8))

    colors = sns.color_palette("crest", n_colors=2)

    ax[0].plot(epochs, accuracy, '-o', color=colors[0])
    ax[0].set_title('Training Accuracy')
    ax[0].set_xlabel('Epochs')
    ax[0].set_ylabel('Accuracy [%]')

    ax[1].plot(epochs, loss, '-o', color=colors[1])
    ax[1].set_title('Training Loss')
    ax[1].set_xlabel('Epochs')
    ax[1].set_ylabel('Loss [%]')

    plt.tight_layout()
    plt.show()

plt_accuracy_loss(history)

# Calculate the model accuracy
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy is {accuracy:.2f}')

"""**Code Visualization**

**Calculate and Plot Confusion Matrix**
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true_classes, y_pred_classes)
sns.heatmap(cm, annot=True, fmt="d")
plt.title('Confusion Matrix for NLP Model')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""**comprehensive analysis consider computing precision, recall, and F1-score.**"""

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_true_classes, y_pred_classes, average='macro')
recall = recall_score(y_true_classes, y_pred_classes, average='macro')
f1 = f1_score(y_true_classes, y_pred_classes, average='macro')

print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}')

"""**Visualization of Model Training**"""

def plt_accuracy_loss(history):
    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(accuracy))

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, accuracy, label='Training Accuracy')
    plt.plot(epochs, val_accuracy, label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    plt.show()

plt_accuracy_loss(history)